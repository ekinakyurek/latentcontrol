{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '8'\n",
    "os.chdir('/opt/project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Mapping, Callable\n",
    "import gzip\n",
    "import math\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "from absl import flags\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from transformers import AutoTokenizer\n",
    "from bertviz import head_view, model_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scripts.train_model\n",
    "from scripts.train_model import ReadOffValues, get_tokenizer\n",
    "from accelerate import Accelerator\n",
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "plt.style.use('.mplstyle')\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "plt.rcParams['figure.figsize'] = 12,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = flags.FLAGS\n",
    "try:\n",
    "    flags.DEFINE_string(\n",
    "        \"save_encodings_path\", None, help=\"Where to load embeddings from\"\n",
    "    )\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = (\n",
    "    \"--disable_tqdm \"\n",
    "    \"--max_generation_len 42 \"\n",
    "    \"--gaccum 1 \"\n",
    "    \"--model=EleutherAI/gpt-j-6B \"\n",
    "    \"--model_type=PromptTuningPostfixLM \"\n",
    "    \"--seed=0 \"\n",
    "    \"--expdir=exps/arithmetic_bugfix/seed_0/PromptTuningPostfixLM/EleutherAI/gpt-j-6B/step_10/lr_0.001 \"\n",
    "    \"--logdir=exps/arithmetic_bugfix/seed_0/PromptTuningPostfixLM/EleutherAI/gpt-j-6B/step_10/lr_0.001 \"\n",
    "    \"--learning_rate=0.001 \"\n",
    "    \"--dataset=ArithmethicDataset \"\n",
    "    \"--N_per_digit=90 \"\n",
    "    \"--n_prompt_tokens=10 \"\n",
    "    \"--n_coder_steps=10 \"\n",
    "    \"--resume_from_checkpoint\"\n",
    "    \" exps/arith_what_is/seed_0/PromptTuningPostfixLM/EleutherAI/gpt-j-6B/step_10/lr_0.001/checkpoints/iter-15.pth.tar \"\n",
    "    \"--save_encodings_path\"\n",
    "    \" exps/arith_what_is/seed_0/PromptTuningPostfixLM/EleutherAI/gpt-j-6B/step_10/lr_0.001/checkpoints/train_reads.pickle \"\n",
    ")\n",
    "sys.argv = sys.argv[:11] + args.split(\" \")\n",
    "FLAGS._parse_args(sys.argv, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(FLAGS.save_encodings_path, 'rb') as handle:\n",
    "    train_reads = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, padding_idx = get_tokenizer(FLAGS.model)\n",
    "FLAGS.padding_idx = padding_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_to_tokens(read: ReadOffValues, \n",
    "                   tokenizer, \n",
    "                   n_prompt_tokens:int=10, \n",
    "                   n_coder_steps:int=10) -> Tuple[torch.tensor, torch.tensor, List[torch.tensor]]:\n",
    "    \n",
    "    input = tokenizer.convert_ids_to_tokens(read.input)\n",
    "    input = (['pre'+str(i) for i in range(n_prompt_tokens)] + \n",
    "             input + \n",
    "             ['post'+str(i) for i in range(n_coder_steps)])\n",
    "    output = tokenizer.convert_ids_to_tokens(read.output)\n",
    "    attentions = [torch.from_numpy(read.attentions[0][k][None, :, :, :]) \n",
    "                  for k in range(len(read.attentions[0]))]\n",
    "    return input, output, attentions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, output, attentions = read_to_tokens(\n",
    "    train_reads[1],\n",
    "    tokenizer,\n",
    "    n_prompt_tokens=FLAGS.n_prompt_tokens,\n",
    "    n_coder_steps=FLAGS.n_coder_steps,\n",
    ")\n",
    "\n",
    "head_view(attentions, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbingDataset(Dataset):\n",
    "    split_ratios: List = [(\"train\", 1.0), (\"dev\", 0.0), (\"test\", 0.0)]\n",
    "\n",
    "    def __init__(self, reads: List[ReadOffValues], **kwargs):\n",
    "        for (k, v) in kwargs.items():\n",
    "            self.__setattr__(k, v)\n",
    "\n",
    "        self.data = self.get_data(reads, **kwargs)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def count_carry(self, a, b):\n",
    "        carry = 0\n",
    "        count = 0\n",
    "        # Initialize len_a and len_b\n",
    "        # with the sizes of strings\n",
    "        len_a = len(a)\n",
    "        len_b = len(b)\n",
    "\n",
    "        carry_ons = []\n",
    "        values = []\n",
    "\n",
    "        while len_a != 0 or len_b != 0:\n",
    "            # Assigning the ascii value\n",
    "            # of the character\n",
    "            x = 0\n",
    "            y = 0\n",
    "            if len_a > 0:\n",
    "                x = int(a[len_a - 1]) + int(\"0\")\n",
    "                len_a -= 1\n",
    "\n",
    "            if len_b > 0:\n",
    "                y = int(b[len_b - 1]) + int(\"0\")\n",
    "                len_b -= 1\n",
    "\n",
    "            # Add both numbers/digits\n",
    "            sum = x + y + carry\n",
    "\n",
    "            # If sum > 0, increment count\n",
    "            # and set carry to 1\n",
    "            if sum >= 10:\n",
    "                carry = 1\n",
    "                count += 1\n",
    "                value = sum % 10\n",
    "            # Else, set carry to 0\n",
    "            else:\n",
    "                carry = 0\n",
    "                value = sum\n",
    "\n",
    "            carry_ons.append(carry)\n",
    "            values.append(value)\n",
    "\n",
    "        return carry_ons, values\n",
    "\n",
    "    def extract_data(\n",
    "        self, reads, seed: int = 0, layer: int = -1, split: str = \"train\"\n",
    "    ):\n",
    "        print(f\"Layer: {layer}\")\n",
    "        rng = np.random.default_rng(seed)\n",
    "        rng.shuffle(reads)\n",
    "        data = []\n",
    "        for read in reads:\n",
    "            hiddens = read.hidden_states[0][layer]\n",
    "            x1, x2 = read.input_str.replace(\"Q: What is \", \"\").replace(\"?\\n\", \"\").split(\" plus \")\n",
    "            x1 = x1.replace(\" \", \"\")\n",
    "            x2 = x2.replace(\" =\", \"\").replace(\" \", \"\")\n",
    "            carry_ons, values = self.count_carry(x1, x2)\n",
    "            data.append((hiddens, carry_ons, values, read.input))\n",
    "        return data\n",
    "\n",
    "    def get_split(self, examples, split: str = \"train\"):\n",
    "        L = len(examples)\n",
    "        data = {}\n",
    "        index = 0\n",
    "        for (i, (split_name, ratio)) in enumerate(self.split_ratios):\n",
    "            length = math.floor(L * ratio)\n",
    "            if i != len(self.split_ratios) - 1:\n",
    "                end_index = min(index + length, L)\n",
    "            else:\n",
    "                end_index = L\n",
    "            data[split_name] = examples[index:end_index]\n",
    "            index = end_index\n",
    "\n",
    "        return data[split]\n",
    "\n",
    "    def get_data(\n",
    "        self, reads, split: str = \"train\", seed=0, **kwargs\n",
    "    ) -> List[Tuple]:\n",
    "        seed, new_seed = utils.split_seed(seed)\n",
    "        examples = self.extract_data(reads, seed=new_seed, **kwargs)\n",
    "        examples = self.get_split(examples, split=split)\n",
    "        seed, new_seed = utils.split_seed(seed)\n",
    "        rng = np.random.default_rng(new_seed)\n",
    "        rng.shuffle(examples)\n",
    "        return examples\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple:\n",
    "        data = self.data[index]\n",
    "        return tuple(map(torch.tensor, data))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_collate(\n",
    "        pad_token_id: int = -100,\n",
    "        pre_token_id: int = -1,\n",
    "        post_token_id: int = -2,\n",
    "        n_prompt_tokens: int = 0,\n",
    "        n_coder_steps: int = 0,\n",
    "    ) -> Callable:\n",
    "        def collate(data) -> Mapping[str, torch.Tensor]:\n",
    "            carry_ons = [d[1] for d in data]\n",
    "            values = [d[2] for d in data]\n",
    "            carry_ons = pad_sequence(\n",
    "                carry_ons, padding_value=-100, batch_first=True\n",
    "            ).long()\n",
    "\n",
    "            values = pad_sequence(\n",
    "                values, padding_value=-100, batch_first=True\n",
    "            ).long()\n",
    "\n",
    "            hiddens = pad_sequence(\n",
    "                [d[0] for d in data], batch_first=True\n",
    "            ).float()\n",
    "            \n",
    "            inputs = pad_sequence(\n",
    "                [\n",
    "                    torch.concat(\n",
    "                        [\n",
    "                            torch.full((n_prompt_tokens,), pre_token_id),\n",
    "                            d[3],\n",
    "                            torch.full((n_coder_steps,), post_token_id),\n",
    "                        ],\n",
    "                        dim=0,\n",
    "                    )\n",
    "                    for d in data\n",
    "                ],\n",
    "                batch_first=True,\n",
    "                padding_value=pad_token_id,\n",
    "            ).long()\n",
    "\n",
    "            mask = inputs == pad_token_id\n",
    "\n",
    "            data = {\n",
    "                \"carry_ons\": carry_ons,\n",
    "                \"values\": values,\n",
    "                \"hiddens\": hiddens,\n",
    "                \"mask\": mask,\n",
    "                \"inputs\": inputs,\n",
    "            }\n",
    "\n",
    "            return data\n",
    "\n",
    "        return collate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(FLAGS.save_encodings_path.replace(\"train\", \"val\"), 'rb') as handle:\n",
    "    val_reads = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = ProbingDataset(train_reads, layer=-1)\n",
    "val_dataset = ProbingDataset(val_reads, layer=-1)\n",
    "collate_fn = ProbingDataset.get_collate(pad_token_id=tokenizer.pad_token_id,\n",
    "                                        pre_token_id=tokenizer.vocab['pre'],\n",
    "                                        post_token_id=tokenizer.vocab['post'],\n",
    "                                        n_coder_steps=FLAGS.n_coder_steps,\n",
    "                                        n_prompt_tokens=FLAGS.n_prompt_tokens)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features,\n",
    "        n_hidden,\n",
    "        key=False,\n",
    "        query=True,\n",
    "        memory=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.key = key\n",
    "        self.query = query\n",
    "        self.memory = memory\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        if self.key:\n",
    "            self.make_key = nn.Linear(n_features, n_hidden)\n",
    "        if self.query:\n",
    "            self.make_query = nn.Linear(n_features, n_hidden)\n",
    "        if self.memory:\n",
    "            self.make_memory = nn.Linear(n_features, n_hidden)\n",
    "\n",
    "        self.n_out = n_hidden\n",
    "\n",
    "    def forward(self, hidden, features, mask=None):\n",
    "        if self.key:\n",
    "            key = self.make_key(features)\n",
    "        else:\n",
    "            key = features\n",
    "\n",
    "        if self.memory:\n",
    "            memory = self.make_memory(features)\n",
    "        else:\n",
    "            memory = features\n",
    "\n",
    "        if self.query:\n",
    "            query = self.make_query(hidden)\n",
    "        else:\n",
    "            query = hidden\n",
    "\n",
    "        # attention\n",
    "        # query = query.expand_as(key) # B x T x H\n",
    "\n",
    "        query = query.unsqueeze(1).unsqueeze(0)\n",
    "        key = key.unsqueeze(1)\n",
    "\n",
    "        scores = (key * query).sum(dim=-1)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores += (mask.unsqueeze(1) * -99999)\n",
    "\n",
    "        distribution = F.softmax(scores, dim=2)\n",
    "        weighted = memory.unsqueeze(1) * distribution.unsqueeze(-1)\n",
    "        summary = weighted.sum(dim=2, keepdim=False)\n",
    "\n",
    "        # value\n",
    "        return summary, distribution\n",
    "\n",
    "\n",
    "class ProbModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        max_digits: int = 8,\n",
    "        dropout: float = 0.0,\n",
    "        ignore_index: int = -100,\n",
    "        w_init: float = 0.01,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.max_digits = max_digits\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.carry_on_selector = nn.Parameter(\n",
    "            torch.randn(max_digits, hidden_dim) * w_init\n",
    "        )\n",
    "        self.carry_on_projector = nn.Linear(hidden_dim // 4, 10)\n",
    "\n",
    "        self.carry_on_attention = SimpleAttention(\n",
    "            n_features=hidden_dim,\n",
    "            n_hidden=hidden_dim // 4,\n",
    "            key=True,\n",
    "            query=True,\n",
    "            memory=True,\n",
    "        )\n",
    "\n",
    "        self.value_selector = nn.Parameter(\n",
    "            torch.randn(max_digits, hidden_dim) * w_init\n",
    "        )\n",
    "        self.value_projector = nn.Linear(hidden_dim // 4, 10)\n",
    "\n",
    "        self.value_attention = SimpleAttention(\n",
    "            n_features=hidden_dim,\n",
    "            n_hidden=hidden_dim // 4,\n",
    "            key=True,\n",
    "            query=True,\n",
    "            memory=True,\n",
    "        )\n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=ignore_index, reduction='sum')\n",
    "\n",
    "    def forward(self, data, only_loss=False):\n",
    "        hiddens = data[\"hiddens\"]\n",
    "        carry, carry_distribution = self.carry_on_attention(\n",
    "            self.carry_on_selector, hiddens, mask=data[\"mask\"]\n",
    "        )\n",
    "        value, value_distribution = self.carry_on_attention(\n",
    "            self.value_selector, hiddens, mask=data[\"mask\"]\n",
    "        )\n",
    "        carry = self.carry_on_projector(carry).transpose(2, 1)\n",
    "        value = self.value_projector(value).transpose(2, 1)\n",
    "\n",
    "        if only_loss:\n",
    "            carry = carry[..., :data[\"carry_ons\"].shape[-1]]\n",
    "            value = value[..., :data[\"values\"].shape[-1]]\n",
    "                        \n",
    "            return self.loss(carry, data[\"carry_ons\"]) + self.loss(value, data[\"values\"])\n",
    "        else:\n",
    "            return (carry, carry_distribution), (value, value_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProbModel(hidden_dim=4096, max_digits=8)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, train_loader, val_loader = accelerator.prepare(model, optimizer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, accelerator, train_loader, epochs=20):\n",
    "    total_loss = 0.0\n",
    "    total_count = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    model.train()\n",
    "    for iter in range(epochs):\n",
    "        for step, data in enumerate(train_loader):\n",
    "            loss = model(data, only_loss=True)\n",
    "            \n",
    "            token_count = (data[\"carry_ons\"] != -100).sum().item() + (\n",
    "                data[\"values\"] != -100\n",
    "            ).sum()\n",
    "            \n",
    "            loss = loss/token_count\n",
    "            \n",
    "            accelerator.backward(loss)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "   \n",
    "            total_loss += loss.item() * token_count.item()\n",
    "            total_count += token_count.item()\n",
    "\n",
    "        avg_loss = total_loss / total_count\n",
    "        print(f\"train/loss/{iter}: {avg_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(model, val_loader, iter=0):\n",
    "    total_loss = 0.0\n",
    "    total_count = 0.0\n",
    "    total_corrects = 0.0\n",
    "    model.eval()\n",
    "    for step, data in enumerate(val_loader):\n",
    "        (carry, carry_distribution), (value, value_distribution) = model(\n",
    "            data, only_loss=False\n",
    "        )\n",
    "        \n",
    "        carry_preds = carry.argmax(dim=1)[..., :data[\"carry_ons\"].shape[-1]]\n",
    "        value_preds = value.argmax(dim=1)[..., :data[\"values\"].shape[-1]]\n",
    "        \n",
    "        # print(value_preds)\n",
    "        # print(data[\"values\"])\n",
    "                \n",
    "        token_count = (\n",
    "            data[\"values\"] != -100\n",
    "        ).sum().item()  + (data[\"carry_ons\"] != -100).sum().item() \n",
    "         \n",
    "        corrects = (\n",
    "            data[\"values\"] == value_preds\n",
    "         ).sum().item() + (data[\"carry_ons\"] == carry_preds).sum().item() \n",
    "        \n",
    "        # total_loss += (loss.item() * token_count)\n",
    "        total_corrects += corrects\n",
    "        total_count += token_count\n",
    "\n",
    "    # avg_loss = total_loss / total_count\n",
    "    avg_accuracy = total_corrects / total_count\n",
    "    # print(f\"train/loss/{iter}: {avg_loss}\")\n",
    "    print(f\"train/accuracy/{iter}: {avg_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(model, optimizer, accelerator, train_loader, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loop(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_single_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "val_single_loader = accelerator.prepare(val_single_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [d for d in val_single_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id=15\n",
    "data = datas[id]\n",
    "(carry, carry_distribution), (value, value_distribution) = model(\n",
    "                data, only_loss=False\n",
    "            )\n",
    "\n",
    "print(carry_distribution.shape)\n",
    "carry_distribution = carry_distribution[0, ...].cpu().detach().numpy()\n",
    "value_distribution = value_distribution[0, ...].cpu().detach().numpy()\n",
    "input = data['inputs'][0, ...]\n",
    "input_str = [tokenizer.decode(t) for t in input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(data, row_labels, col_labels, title=\"\", ax=None, **kwargs):\n",
    "    ax = sns.heatmap(data, **kwargs)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]))\n",
    "    ax.set_yticks(np.arange(data.shape[0]))\n",
    "    # ... and label them with the respective list entries.\n",
    "    ax.set_xticklabels(col_labels)\n",
    "    ax.set_yticklabels(row_labels)\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=True, bottom=False,\n",
    "                   labeltop=True, labelbottom=False)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\")\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(carry_distribution, [str(i) for i in range(8)], input_str, \"carry prob attentions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(value_distribution, [str(i) for i in range(8)], input_str, \"value prob attentions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
